{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30c4b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#own packages\n",
    "from my_packages.comments_filter import emojies_re_pattern\n",
    "from my_packages.process_timestamps_update import plot_time_stamps\n",
    "from my_packages.download_comments_update import download_comments\n",
    "\n",
    "\n",
    "#standart packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect\n",
    "\n",
    "#machine learning packages\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# https://pypi.org/project/simpletransformers/ <- source evaluation tipps\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from sys import stderr\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Annotation List\n",
    "df1 = pd.read_csv(\"Ed-Sheeran-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df2 = pd.read_csv(\"John-Legend-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df3 = pd.read_csv(\"Kelly-Clarkson-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df4 = pd.read_csv(\"Tom-Odell-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "\n",
    "df1 = df1[df1[\"english\"] == True]\n",
    "print(len(df1))\n",
    "df2 = df2[df2[\"english\"] == True]\n",
    "print(len(df2))\n",
    "df3 = df3[df3[\"english\"] == True]\n",
    "print(len(df3))\n",
    "df4 = df4[df4[\"english\"] == True]\n",
    "print(len(df4))\n",
    "\n",
    "df1 = df1.head(250)\n",
    "df2 = df2.head(250)\n",
    "df3 = df3.head(250)\n",
    "df4 = df4.head(250)\n",
    "\n",
    "df_final = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "print(df_final)\n",
    "filename = \"final_annotation.tsv\"\n",
    "df_final.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import *finished* annotated file\n",
    "filename = \"annotation_data/annotate_1000_final.tsv\"\n",
    "annotations = pd.read_csv(filename, sep=\"\\t\")\n",
    "selected_columns = [\"author\", \"comment\", \"origin\", \"origin_status\", \"category\"]\n",
    "annotations = annotations[selected_columns]\n",
    "print(annotations.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare Dataset\n",
    "#delete non-category\n",
    "annotations = annotations[annotations[\"category\"] != -1]\n",
    "\n",
    "#shuffle data and drop old index\n",
    "annotations = annotations.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# create training-data <-- df\n",
    "train_df = annotations.head(int(len(annotations)*0.8))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "train_df = train_df[selected_columns]\n",
    "\n",
    "# create eval-data <-- df\n",
    "eval_df = annotations.tail(int(len(annotations)*0.2))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "eval_df = eval_df[selected_columns]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model and Save it\n",
    "\n",
    "# toggle logging\n",
    "#from transformers.utils import logging\n",
    "#logging.set_verbosity_info()\n",
    "\n",
    "#TOKENIZERS_PARALLELISM=True\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(\n",
    "                                num_train_epochs=1, \n",
    "                                overwrite_output_dir= True\n",
    "                                )\n",
    "\n",
    "model = ClassificationModel(\n",
    "                            'distilbert',\n",
    "                            'distilbert-base-uncased',\n",
    "                            use_cuda=False,\n",
    "                            args=model_args\n",
    "                            )\n",
    "#--> set cuda to True, if available\n",
    "\n",
    "# start training#\n",
    "model.train_model(train_df)\n",
    "\n",
    "#save model\n",
    "modelname = \"model_p82\"\n",
    "model.model.save_pretrained(modelname)\n",
    "model.tokenizer.save_pretrained(modelname)\n",
    "model.config.save_pretrained(f'{modelname}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "\n",
    "# reload model (if needed)\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "\n",
    "# start evaluation\n",
    "print(\"Starting Evaluation:\")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "# print results\n",
    "print(\"Results:\")\n",
    "for i in result:\n",
    "    print(i, result[i])\n",
    "\n",
    "#tp = true positives\n",
    "#tn = true negatives\n",
    "#fp = false positives\n",
    "#fn = false negaives\n",
    "#auroc = precision\n",
    "#auprc = average precison\n",
    "    \n",
    "print(\"Precision:\")\n",
    "print((len(eval_df)-len(wrong_predictions))/len(eval_df))\n",
    "    \n",
    "print(len(wrong_predictions))\n",
    "\n",
    "# print all wrong preductions (if wanted)\n",
    "#for n, dic in enumerate(wrong_predictions):\n",
    "#    print(\"Annotation Label:\", dic.label)\n",
    "#    print(dic.text_a)\n",
    "#    print(10*'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825858c",
   "metadata": {},
   "source": [
    "# Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6768fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0.2 Unnamed: 0.1 Unnamed: 0            author  \\\n",
      "10274         10274            0          0     Bruce VanBeek   \n",
      "10275         10275            3          3         nayneeeee   \n",
      "10276         10276            4          4  letitia marshall   \n",
      "10277         10277            7          7           soni uk   \n",
      "10278         10278            8          8  Christian Havana   \n",
      "...             ...          ...        ...               ...   \n",
      "21017         21017        20080      20080          ScarWølf   \n",
      "21018         21018        20081      20081            Ronald   \n",
      "21019         21019        20082      20082         Rahaf Mrd   \n",
      "21020         21020        20085      20085           Bela S.   \n",
      "21021         21021        20086      20086    shrooq mujarbi   \n",
      "\n",
      "                                                 comment  \\\n",
      "10274  I know a daughter of a good man . Since her mo...   \n",
      "10275  In every failed relationship it was always the...   \n",
      "10276                                     child helpline   \n",
      "10277                         22/06/2023 United Kingdom    \n",
      "10278                                  My childhood song   \n",
      "...                                                  ...   \n",
      "21017  When you have depression and you listen to thi...   \n",
      "21018      31mil people cried in the middle of the night   \n",
      "21019  I am sorry for my children but I got married a...   \n",
      "21020                      This songs describes my life.   \n",
      "21021  I learned to play on the safe side to not get ...   \n",
      "\n",
      "                              origin  origin_status lang  time_stamp  \n",
      "10274  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10275  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10276  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10277  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10278  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "...                              ...            ...  ...         ...  \n",
      "21017  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21018  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21019  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21020  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21021  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "\n",
      "[10631 rows x 9 columns]\n",
      "10631\n"
     ]
    }
   ],
   "source": [
    "# prepare Application-Dataset (input needs to be a list)\n",
    "filename = \"comment_downloads/data_combined_53221.tsv\"\n",
    "origin = \"Kelly_Clarkson-Because_Of_You\"\n",
    "time_stamp = False\n",
    "\n",
    "\n",
    "apply_df = pd.read_csv(filename, sep=\"\\t\")\n",
    "apply_df = apply_df[apply_df[\"origin\"] == origin]\n",
    "apply_df = apply_df[apply_df[\"time_stamp\"] == time_stamp]\n",
    "\n",
    "\n",
    "\n",
    "print(apply_df)\n",
    "apply_lst = apply_df['comment'].tolist()\n",
    "\n",
    "print(len(apply_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f880b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                    | 1/113 [00:04<09:03,  4.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "\n",
    "#apply model\n",
    "predictions, raw_outputs = model.predict(apply_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fae804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "comment_downloads/data_combined_53221.tsv Kelly_Clarkson-Because_Of_You True\n",
      "1 = schnulzig; 0 = nicht-schnulzig\n",
      "0    89\n",
      "1    24\n",
      "Name: prediction, dtype: int64\n",
      "results saved in comment_downloads/data_combined_53221.tsv\n",
      "21.24% der Kommentare wurden als schnulzig klassifiziert.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.stdout = open('output.txt','a')\n",
    "print(20*\"-\")\n",
    "print(filename, origin, time_stamp)\n",
    "\n",
    "# output and save predictions\n",
    "final = []\n",
    "for i, prediction in enumerate(predictions):\n",
    "  comment_result = (apply_lst[i], str(prediction), str(raw_outputs[i]))\n",
    "  final.append(comment_result)\n",
    "df = pd.DataFrame(final, columns =['txt', 'prediction', 'probablility'])\n",
    "\n",
    "#filename = 'output.tsv'\n",
    "#df.to_csv(filename, sep='\\t')\n",
    "counts = df[\"prediction\"].value_counts()\n",
    "print(\"1 = schnulzig; 0 = nicht-schnulzig\")\n",
    "print(counts)\n",
    "print(f\"results saved in {filename}\")\n",
    "\n",
    "perc = counts.iloc[1]/(counts.iloc[0]+counts.iloc[1])\n",
    "perc = '{:,.2%}'.format(perc)\n",
    "print(f\"{perc} der Kommentare wurden als schnulzig klassifiziert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a53ceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_p82/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_p82/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_p82.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                | 0/1 [00:00<?, ?it/s]Process SpawnPoolWorker-105:\n",
      "Process SpawnPoolWorker-109:\n",
      "Process SpawnPoolWorker-103:\n",
      "Process SpawnPoolWorker-108:\n",
      "Process SpawnPoolWorker-107:\n",
      "Process SpawnPoolWorker-104:\n",
      "Process SpawnPoolWorker-101:\n",
      "Process SpawnPoolWorker-106:\n",
      "Process SpawnPoolWorker-110:\n",
      "  0%|                                                                                                                                                                                                | 0/1 [00:05<?, ?it/s]Process SpawnPoolWorker-102:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/a\n",
      "naconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m comment \u001b[38;5;241m=\u001b[39m single_lst\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 47\u001b[0m predictions, raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m com_lst\u001b[38;5;241m.\u001b[39mappend(comment)\n\u001b[1;32m     49\u001b[0m pred_lst\u001b[38;5;241m.\u001b[39mappend(predictions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:2087\u001b[0m, in \u001b[0;36mClassificationModel.predict\u001b[0;34m(self, to_predict, multi_label)\u001b[0m\n\u001b[1;32m   2085\u001b[0m         out_label_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(eval_dataset)))\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2087\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m eval_sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(eval_dataset)\n\u001b[1;32m   2092\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m   2093\u001b[0m     eval_dataset, sampler\u001b[38;5;241m=\u001b[39meval_sampler, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size\n\u001b[1;32m   2094\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1827\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1827\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1833\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[38;5;241m+\u001b[39m chunksize], \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer, args\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(args\u001b[38;5;241m.\u001b[39mprocess_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_data_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     examples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[38;5;241m.\u001b[39mcat([example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import sys\n",
    "#machine learning packages\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# https://pypi.org/project/simpletransformers/ <- source evaluation tipps\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from sys import stderr\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "origin = \"Kelly_Clarkson-Because_Of_You\"\n",
    "time_stamp = False\n",
    "\n",
    "\n",
    "\n",
    "# prepare Application-Dataset (input needs to be a list)\n",
    "filename = \"comment_downloads/data_combined_53221.tsv\"\n",
    "apply_df = pd.read_csv(filename, sep=\"\\t\")\n",
    "apply_df = apply_df[apply_df[\"origin\"] == origin]\n",
    "apply_df = apply_df[apply_df[\"time_stamp\"] == time_stamp]\n",
    "apply_lst = apply_df['comment'].tolist()\n",
    "apply_lst = apply_lst[:5]\n",
    "\n",
    "print(len(apply_lst))\n",
    "# load model\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "#apply model\n",
    "\n",
    "com_lst = []\n",
    "pred_lst = []\n",
    "\n",
    "for i, comment in enumerate(apply_lst):\n",
    "    single_lst = []\n",
    "    single_lst.append(comment)\n",
    "    comment = single_lst\n",
    "    print(i)\n",
    "    predictions, raw_outputs = model.predict(comment)\n",
    "    com_lst.append(comment)\n",
    "    pred_lst.append(predictions)\n",
    "\n",
    "print(20*\"-\")\n",
    "print(filename, origin, time_stamp)\n",
    "\n",
    "# output and save predictions\n",
    "final = []\n",
    "for i, comment in enumerate(com_lst):\n",
    "  comment_row = (comment, str(pred_lst[i]))\n",
    "  final.append(comment_row)\n",
    "\n",
    "df = pd.DataFrame(final, columns =['txt', 'prediction'])\n",
    "\n",
    "print(\"1 = schnulzig; 0 = nicht-schnulzig\")\n",
    "counts = df[\"prediction\"].value_counts()\n",
    "print(counts)\n",
    "try:\n",
    "    perc = counts.iloc[1]/(counts.iloc[0]+counts.iloc[1])\n",
    "except:\n",
    "    perc = 0\n",
    "perc = '{:,.2%}'.format(perc)\n",
    "print(f\"{perc} der Kommentare wurden als schnulzig klassifiziert.\")\n",
    "\n",
    "\n",
    "filename = 'output.tsv'\n",
    "df.to_csv(filename, sep='\\t')\n",
    "print(f\"results saved in {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_ml_env2)",
   "language": "python",
   "name": "my_ml_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
