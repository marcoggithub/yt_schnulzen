{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30c4b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#jupyter\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#own packages\n",
    "#from my_packages.comments_filter import emojies_re_pattern\n",
    "#from my_packages.process_timestamps_update import plot_time_stamps\n",
    "#from my_packages.download_comments_update import download_comments\n",
    "\n",
    "\n",
    "#standart packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect\n",
    "\n",
    "#machine learning packages\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# https://pypi.org/project/simpletransformers/ <- source evaluation tipps\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from sys import stderr\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Annotation List\n",
    "df1 = pd.read_csv(\"Ed-Sheeran-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df2 = pd.read_csv(\"John-Legend-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df3 = pd.read_csv(\"Kelly-Clarkson-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "df4 = pd.read_csv(\"Tom-Odell-1000-PP.tsv\", sep=\"\\t\", nrows=500)\n",
    "\n",
    "df1 = df1[df1[\"english\"] == True]\n",
    "print(len(df1))\n",
    "df2 = df2[df2[\"english\"] == True]\n",
    "print(len(df2))\n",
    "df3 = df3[df3[\"english\"] == True]\n",
    "print(len(df3))\n",
    "df4 = df4[df4[\"english\"] == True]\n",
    "print(len(df4))\n",
    "\n",
    "df1 = df1.head(250)\n",
    "df2 = df2.head(250)\n",
    "df3 = df3.head(250)\n",
    "df4 = df4.head(250)\n",
    "\n",
    "df_final = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "print(df_final)\n",
    "filename = \"final_annotation.tsv\"\n",
    "df_final.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7dc28b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['author', 'comment', 'origin', 'origin_status', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## import *finished* annotated file\n",
    "filename = \"annotation_data/annotate_1000_final.tsv\"\n",
    "annotations = pd.read_csv(filename, sep=\"\\t\")\n",
    "selected_columns = [\"author\", \"comment\", \"origin\", \"origin_status\", \"category\"]\n",
    "annotations = annotations[selected_columns]\n",
    "print(annotations.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cd2b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "188\n"
     ]
    }
   ],
   "source": [
    "## prepare Dataset\n",
    "#delete non-category\n",
    "annotations = annotations[annotations[\"category\"] != -1]\n",
    "\n",
    "#shuffle data and drop old index\n",
    "annotations = annotations.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# create training-data <-- df\n",
    "train_df = annotations.head(int(len(annotations)*0.8))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "train_df = train_df[selected_columns]\n",
    "\n",
    "# create eval-data <-- df\n",
    "eval_df = annotations.tail(int(len(annotations)*0.2))\n",
    "selected_columns = [\"comment\", \"category\"]\n",
    "eval_df = eval_df[selected_columns]\n",
    "\n",
    "print(len(train_df))\n",
    "print(len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7279a3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  0%|▌                                                                                                                                                                                              | 2/752 [00:05<32:59,  2.64s/it]\n",
      "Epoch 1 of 1:   0%|                                                                                                                                                                                           | 0/1 [00:00<?, ?it/s]\n",
      "Running Epoch 0 of 1:   0%|                                                                                                                                                                                  | 0/94 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7241:   0%|                                                                                                                                                                   | 0/94 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7241:   1%|█▋                                                                                                                                                         | 1/94 [00:01<02:58,  1.91s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7080:   1%|█▋                                                                                                                                                         | 1/94 [00:02<02:58,  1.91s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7080:   2%|███▎                                                                                                                                                       | 2/94 [00:03<02:24,  1.57s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7064:   2%|███▎                                                                                                                                                       | 2/94 [00:03<02:24,  1.57s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7064:   3%|████▉                                                                                                                                                      | 3/94 [00:04<02:13,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6728:   3%|████▉                                                                                                                                                      | 3/94 [00:04<02:13,  1.46s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6728:   4%|██████▌                                                                                                                                                    | 4/94 [00:05<02:08,  1.42s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6842:   4%|██████▌                                                                                                                                                    | 4/94 [00:06<02:08,  1.42s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6842:   5%|████████▏                                                                                                                                                  | 5/94 [00:07<02:03,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6718:   5%|████████▏                                                                                                                                                  | 5/94 [00:07<02:03,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6718:   6%|█████████▉                                                                                                                                                 | 6/94 [00:08<02:00,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6250:   6%|█████████▉                                                                                                                                                 | 6/94 [00:08<02:00,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6250:   7%|███████████▌                                                                                                                                               | 7/94 [00:09<01:58,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5359:   7%|███████████▌                                                                                                                                               | 7/94 [00:10<01:58,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5359:   9%|█████████████▏                                                                                                                                             | 8/94 [00:11<01:56,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4776:   9%|█████████████▏                                                                                                                                             | 8/94 [00:11<01:56,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4776:  10%|██████████████▊                                                                                                                                            | 9/94 [00:12<01:54,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4746:  10%|██████████████▊                                                                                                                                            | 9/94 [00:12<01:54,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4746:  11%|████████████████▍                                                                                                                                         | 10/94 [00:13<01:52,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8237:  11%|████████████████▍                                                                                                                                         | 10/94 [00:14<01:52,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8237:  12%|██████████████████                                                                                                                                        | 11/94 [00:15<01:51,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5426:  12%|██████████████████                                                                                                                                        | 11/94 [00:15<01:51,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5426:  13%|███████████████████▋                                                                                                                                      | 12/94 [00:16<01:50,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5205:  13%|███████████████████▋                                                                                                                                      | 12/94 [00:16<01:50,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5205:  14%|█████████████████████▎                                                                                                                                    | 13/94 [00:17<01:48,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7016:  14%|█████████████████████▎                                                                                                                                    | 13/94 [00:18<01:48,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7016:  15%|██████████████████████▉                                                                                                                                   | 14/94 [00:19<01:47,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6833:  15%|██████████████████████▉                                                                                                                                   | 14/94 [00:19<01:47,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6833:  16%|████████████████████████▌                                                                                                                                 | 15/94 [00:20<01:45,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6663:  16%|████████████████████████▌                                                                                                                                 | 15/94 [00:20<01:45,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6663:  17%|██████████████████████████▏                                                                                                                               | 16/94 [00:21<01:44,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5059:  17%|██████████████████████████▏                                                                                                                               | 16/94 [00:22<01:44,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5059:  18%|███████████████████████████▊                                                                                                                              | 17/94 [00:23<01:42,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8847:  18%|███████████████████████████▊                                                                                                                              | 17/94 [00:23<01:42,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8847:  19%|█████████████████████████████▍                                                                                                                            | 18/94 [00:24<01:41,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4256:  19%|█████████████████████████████▍                                                                                                                            | 18/94 [00:24<01:41,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4256:  20%|███████████████████████████████▏                                                                                                                          | 19/94 [00:25<01:40,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5501:  20%|███████████████████████████████▏                                                                                                                          | 19/94 [00:26<01:40,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5501:  21%|████████████████████████████████▊                                                                                                                         | 20/94 [00:27<01:39,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5813:  21%|████████████████████████████████▊                                                                                                                         | 20/94 [00:27<01:39,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5813:  22%|██████████████████████████████████▍                                                                                                                       | 21/94 [00:28<01:38,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5337:  22%|██████████████████████████████████▍                                                                                                                       | 21/94 [00:28<01:38,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5337:  23%|████████████████████████████████████                                                                                                                      | 22/94 [00:30<01:36,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5813:  23%|████████████████████████████████████                                                                                                                      | 22/94 [00:30<01:36,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5813:  24%|█████████████████████████████████████▋                                                                                                                    | 23/94 [00:31<01:34,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5812:  24%|█████████████████████████████████████▋                                                                                                                    | 23/94 [00:31<01:34,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5812:  26%|███████████████████████████████████████▎                                                                                                                  | 24/94 [00:32<01:33,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4741:  26%|███████████████████████████████████████▎                                                                                                                  | 24/94 [00:32<01:33,  1.33s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4741:  27%|████████████████████████████████████████▉                                                                                                                 | 25/94 [00:33<01:32,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4446:  27%|████████████████████████████████████████▉                                                                                                                 | 25/94 [00:34<01:32,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4446:  28%|██████████████████████████████████████████▌                                                                                                               | 26/94 [00:35<01:31,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5839:  28%|██████████████████████████████████████████▌                                                                                                               | 26/94 [00:35<01:31,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5839:  29%|████████████████████████████████████████████▏                                                                                                             | 27/94 [00:36<01:30,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4298:  29%|████████████████████████████████████████████▏                                                                                                             | 27/94 [00:37<01:30,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4298:  30%|█████████████████████████████████████████████▊                                                                                                            | 28/94 [00:38<01:28,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4726:  30%|█████████████████████████████████████████████▊                                                                                                            | 28/94 [00:38<01:28,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4726:  31%|███████████████████████████████████████████████▌                                                                                                          | 29/94 [00:39<01:27,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3113:  31%|███████████████████████████████████████████████▌                                                                                                          | 29/94 [00:39<01:27,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3113:  32%|█████████████████████████████████████████████████▏                                                                                                        | 30/94 [00:40<01:25,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4441:  32%|█████████████████████████████████████████████████▏                                                                                                        | 30/94 [00:41<01:25,  1.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0/1. Running Loss:    0.4441:  33%|██████████████████████████████████████████████████▊                                                                                                       | 31/94 [00:42<01:24,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4693:  33%|██████████████████████████████████████████████████▊                                                                                                       | 31/94 [00:42<01:24,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4693:  34%|████████████████████████████████████████████████████▍                                                                                                     | 32/94 [00:43<01:23,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1237:  34%|████████████████████████████████████████████████████▍                                                                                                     | 32/94 [00:43<01:23,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1237:  35%|██████████████████████████████████████████████████████                                                                                                    | 33/94 [00:44<01:21,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    1.3696:  35%|██████████████████████████████████████████████████████                                                                                                    | 33/94 [00:45<01:21,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    1.3696:  36%|███████████████████████████████████████████████████████▋                                                                                                  | 34/94 [00:46<01:20,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6369:  36%|███████████████████████████████████████████████████████▋                                                                                                  | 34/94 [00:46<01:20,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6369:  37%|█████████████████████████████████████████████████████████▎                                                                                                | 35/94 [00:47<01:18,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6184:  37%|█████████████████████████████████████████████████████████▎                                                                                                | 35/94 [00:47<01:18,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6184:  38%|██████████████████████████████████████████████████████████▉                                                                                               | 36/94 [00:48<01:17,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4445:  38%|██████████████████████████████████████████████████████████▉                                                                                               | 36/94 [00:49<01:17,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4445:  39%|████████████████████████████████████████████████████████████▌                                                                                             | 37/94 [00:50<01:16,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4063:  39%|████████████████████████████████████████████████████████████▌                                                                                             | 37/94 [00:50<01:16,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4063:  40%|██████████████████████████████████████████████████████████████▎                                                                                           | 38/94 [00:51<01:15,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3807:  40%|██████████████████████████████████████████████████████████████▎                                                                                           | 38/94 [00:51<01:15,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3807:  41%|███████████████████████████████████████████████████████████████▉                                                                                          | 39/94 [00:52<01:13,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4564:  41%|███████████████████████████████████████████████████████████████▉                                                                                          | 39/94 [00:53<01:13,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4564:  43%|█████████████████████████████████████████████████████████████████▌                                                                                        | 40/94 [00:54<01:14,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3255:  43%|█████████████████████████████████████████████████████████████████▌                                                                                        | 40/94 [00:54<01:14,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3255:  44%|███████████████████████████████████████████████████████████████████▏                                                                                      | 41/94 [00:55<01:13,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4541:  44%|███████████████████████████████████████████████████████████████████▏                                                                                      | 41/94 [00:55<01:13,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4541:  45%|████████████████████████████████████████████████████████████████████▊                                                                                     | 42/94 [00:56<01:11,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6200:  45%|████████████████████████████████████████████████████████████████████▊                                                                                     | 42/94 [00:57<01:11,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6200:  46%|██████████████████████████████████████████████████████████████████████▍                                                                                   | 43/94 [00:58<01:09,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4684:  46%|██████████████████████████████████████████████████████████████████████▍                                                                                   | 43/94 [00:58<01:09,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4684:  47%|████████████████████████████████████████████████████████████████████████                                                                                  | 44/94 [00:59<01:09,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5212:  47%|████████████████████████████████████████████████████████████████████████                                                                                  | 44/94 [01:00<01:09,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5212:  48%|█████████████████████████████████████████████████████████████████████████▋                                                                                | 45/94 [01:01<01:07,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4419:  48%|█████████████████████████████████████████████████████████████████████████▋                                                                                | 45/94 [01:01<01:07,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4419:  49%|███████████████████████████████████████████████████████████████████████████▎                                                                              | 46/94 [01:02<01:05,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4331:  49%|███████████████████████████████████████████████████████████████████████████▎                                                                              | 46/94 [01:02<01:05,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4331:  50%|█████████████████████████████████████████████████████████████████████████████                                                                             | 47/94 [01:03<01:03,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3291:  50%|█████████████████████████████████████████████████████████████████████████████                                                                             | 47/94 [01:04<01:03,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3291:  51%|██████████████████████████████████████████████████████████████████████████████▋                                                                           | 48/94 [01:05<01:02,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3752:  51%|██████████████████████████████████████████████████████████████████████████████▋                                                                           | 48/94 [01:05<01:02,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3752:  52%|████████████████████████████████████████████████████████████████████████████████▎                                                                         | 49/94 [01:06<01:00,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3002:  52%|████████████████████████████████████████████████████████████████████████████████▎                                                                         | 49/94 [01:06<01:00,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3002:  53%|█████████████████████████████████████████████████████████████████████████████████▉                                                                        | 50/94 [01:07<00:59,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6941:  53%|█████████████████████████████████████████████████████████████████████████████████▉                                                                        | 50/94 [01:08<00:59,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6941:  54%|███████████████████████████████████████████████████████████████████████████████████▌                                                                      | 51/94 [01:09<00:57,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8358:  54%|███████████████████████████████████████████████████████████████████████████████████▌                                                                      | 51/94 [01:09<00:57,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8358:  55%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 52/94 [01:10<00:56,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3075:  55%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 52/94 [01:10<00:56,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3075:  56%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 53/94 [01:11<00:54,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.9160:  56%|██████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 53/94 [01:12<00:54,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.9160:  57%|████████████████████████████████████████████████████████████████████████████████████████▍                                                                 | 54/94 [01:13<00:53,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3425:  57%|████████████████████████████████████████████████████████████████████████████████████████▍                                                                 | 54/94 [01:13<00:53,  1.34s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3425:  59%|██████████████████████████████████████████████████████████████████████████████████████████                                                                | 55/94 [01:14<00:52,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5368:  59%|██████████████████████████████████████████████████████████████████████████████████████████                                                                | 55/94 [01:14<00:52,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5368:  60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                              | 56/94 [01:16<00:53,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3646:  60%|███████████████████████████████████████████████████████████████████████████████████████████▋                                                              | 56/94 [01:16<00:53,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3646:  61%|█████████████████████████████████████████████████████████████████████████████████████████████▍                                                            | 57/94 [01:17<00:51,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    1.0526:  61%|█████████████████████████████████████████████████████████████████████████████████████████████▍                                                            | 57/94 [01:17<00:51,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    1.0526:  62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                           | 58/94 [01:18<00:50,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1046:  62%|███████████████████████████████████████████████████████████████████████████████████████████████                                                           | 58/94 [01:19<00:50,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1046:  63%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 59/94 [01:20<00:48,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4482:  63%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                         | 59/94 [01:20<00:48,  1.40s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4482:  64%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 60/94 [01:21<00:47,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1548:  64%|██████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 60/94 [01:21<00:47,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1548:  65%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 61/94 [01:22<00:45,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4966:  65%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                      | 61/94 [01:23<00:45,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4966:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 62/94 [01:24<00:44,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1428:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 62/94 [01:24<00:44,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1428:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 63/94 [01:25<00:42,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4020:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 63/94 [01:26<00:42,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4020:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 64/94 [01:27<00:41,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3109:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 64/94 [01:27<00:41,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3109:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 65/94 [01:28<00:39,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3025:  69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 65/94 [01:28<00:39,  1.36s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 0/1. Running Loss:    0.3025:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 66/94 [01:29<00:37,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4871:  70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 66/94 [01:30<00:37,  1.35s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4871:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                            | 67/94 [01:31<00:36,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4499:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                            | 67/94 [01:31<00:36,  1.36s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4499:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 68/94 [01:32<00:35,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4917:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                          | 68/94 [01:32<00:35,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4917:  73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                         | 69/94 [01:33<00:34,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5302:  73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                         | 69/94 [01:34<00:34,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.5302:  74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 70/94 [01:35<00:33,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1725:  74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 70/94 [01:35<00:33,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1725:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 71/94 [01:36<00:31,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3197:  76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 71/94 [01:37<00:31,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3197:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 72/94 [01:38<00:30,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4667:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                    | 72/94 [01:38<00:30,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4667:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 73/94 [01:39<00:28,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1535:  78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 73/94 [01:39<00:28,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1535:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 74/94 [01:40<00:27,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2115:  79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 74/94 [01:41<00:27,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2115:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                               | 75/94 [01:42<00:26,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1992:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                               | 75/94 [01:42<00:26,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1992:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 76/94 [01:43<00:24,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7644:  81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                             | 76/94 [01:43<00:24,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.7644:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 77/94 [01:44<00:23,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2255:  82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 77/94 [01:45<00:23,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2255:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 78/94 [01:46<00:22,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4149:  83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 78/94 [01:46<00:22,  1.39s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4149:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 79/94 [01:47<00:20,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2091:  84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                        | 79/94 [01:48<00:20,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2091:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 80/94 [01:49<00:19,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4687:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 80/94 [01:49<00:19,  1.38s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4687:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 81/94 [01:50<00:17,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2477:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 81/94 [01:50<00:17,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2477:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 82/94 [01:51<00:16,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1077:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 82/94 [01:52<00:16,  1.37s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1077:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 83/94 [01:53<00:15,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2108:  88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 83/94 [01:53<00:15,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.2108:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 84/94 [01:54<00:14,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1735:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                | 84/94 [01:55<00:14,  1.48s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1735:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 85/94 [01:56<00:13,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4254:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎              | 85/94 [01:56<00:13,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4254:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 86/94 [01:57<00:11,  1.44s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8782:  91%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 86/94 [01:58<00:11,  1.44s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8782:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 87/94 [01:59<00:09,  1.42s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3800:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌           | 87/94 [01:59<00:09,  1.42s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3800:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 88/94 [02:00<00:08,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3319:  94%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏         | 88/94 [02:01<00:08,  1.47s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.3319:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 89/94 [02:02<00:07,  1.45s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1681:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 89/94 [02:02<00:07,  1.45s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.1681:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 90/94 [02:03<00:05,  1.43s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8614:  96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍      | 90/94 [02:03<00:05,  1.43s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.8614:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 91/94 [02:04<00:04,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4188:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 91/94 [02:05<00:04,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4188:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 92/94 [02:06<00:02,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6914:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋   | 92/94 [02:06<00:02,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.6914:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [02:07<00:01,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4029:  99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [02:08<00:01,  1.41s/it]\u001b[A\n",
      "Epochs 0/1. Running Loss:    0.4029: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [02:09<00:00,  1.37s/it]\u001b[A\n",
      "Epoch 1 of 1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [02:09<00:00, 129.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94, 0.4837440553497761)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Model and Save it\n",
    "\n",
    "# toggle logging\n",
    "#from transformers.utils import logging\n",
    "#logging.set_verbosity_info()\n",
    "\n",
    "#TOKENIZERS_PARALLELISM=True\n",
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(\n",
    "                                num_train_epochs=1, \n",
    "                                overwrite_output_dir= True\n",
    "                                )\n",
    "\n",
    "model = ClassificationModel(\n",
    "                            'distilbert',\n",
    "                            'distilbert-base-uncased',\n",
    "                            use_cuda=False,\n",
    "                            args=model_args\n",
    "                            )\n",
    "#--> set cuda to True, if available\n",
    "\n",
    "# start training#\n",
    "model.train_model(train_df)\n",
    "\n",
    "#save model\n",
    "#modelname = \"model_p82\"\n",
    "#model.model.save_pretrained(modelname)\n",
    "#model.tokenizer.save_pretrained(modelname)\n",
    "#model.config.save_pretrained(f'{modelname}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd8a7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n",
      "  1%|█                                                                                                                                                                                              | 1/188 [00:04<14:07,  4.53s/it]\n",
      "Running Evaluation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:05<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "mcc 0.5251765486267972\n",
      "tp 29\n",
      "tn 126\n",
      "fp 13\n",
      "fn 20\n",
      "auroc 0.8684480986639259\n",
      "auprc 0.6599662218317867\n",
      "eval_loss 0.40511993194619816\n",
      "Precision:\n",
      "0.824468085106383\n",
      "33\n",
      "Annotation Label: 1\n",
      "When you are happy you enjoy the song and when you’re sad you understand the lyrics\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "Lyrics ....\n",
      "\n",
      "When your legs don't work like they used to before\n",
      "And I can't sweep you off of your feet\n",
      "Will your mouth still remember the taste of my love\n",
      "Will your eyes still smile from your cheeks\n",
      "And darling I will be loving you 'til we're 70\n",
      "And baby my heart could still fall as hard at 23\n",
      "And I'm thinking 'bout how people fall in love in mysterious ways\n",
      "Maybe just the touch of a hand\n",
      "Oh me I fall in love with you every single day\n",
      "And I just wanna tell you I am\n",
      "So honey now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "Maybe we found love right where we are\n",
      "When my hair's all but gone and my memory fades\n",
      "And the crowds don't remember my name\n",
      "When my hands don't play the strings the same way, mm\n",
      "I know you will still love me the same\n",
      "'Cause honey your soul can never grow old, it's evergreen\n",
      "Baby your smile's forever in my mind and memory\n",
      "I'm thinking 'bout how people fall in love in mysterious ways\n",
      "Maybe it's all part of a plan\n",
      "I'll just keep on making the same mistakes\n",
      "Hoping that you'll understand\n",
      "But baby now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "That maybe we found love right where we are, oh\n",
      "So baby now\n",
      "Take me into your loving arms\n",
      "Kiss me under the light of a thousand stars\n",
      "Oh darling, place your head on my beating heart\n",
      "I'm thinking out loud\n",
      "That maybe we found love right where we are\n",
      "Oh baby, we found love right where we are (maybe)\n",
      "And we found love right where we are\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Perfect song for wedding \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "can we talk about the music video for a sec?\n",
      "how he sits there and looks straight into the camera, while the woman literally destroys the room. I interpret it that way, that Tom is so distracted and dissociated because of his past experience and pain, that he cannot worship his new love. he wants to, he also sings, that he'll protect her and if anyone says somethig bad he'll use his voice and \"be so fucking rude\". but yet he cannot give her what he gave his past love. so he lets her destroy everything. he maybe wants to stop it, but can't.\n",
      "\n",
      "the woman on the other hand, wants to try to reach him. she wants be there for him and listen. more specifically, she wants to love and be loved. eventually she loses her patience and screams at him. she starts to destroy things out of rage and hurt. because she loves him, but she feels like he doesn't love her. like he doesn't listen or care about her. they're both hurt. no matter what she says or does, Tom just doesn't answer her. \n",
      "He looks so numb, she gives up and leaves.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Wow, she dances so beautifully, I wish I would dance like that with someone \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "17 years deep into a marriage with two teenagers. My love for my wife is depicted here. Still!\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I can’t believe this song is 7 years old feels like yesterday.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "*THIS SONG REMİS ME FROM MY PAST DAYS OR A PERFECT SONG I CAN'T EXPRESS MYSELF SO TİME FLOWS SO FAST...*\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I Dying of love for Ed Sheeran\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "2013: Heard this song for the first time. \r\n",
      "2014: Showed my girlfriend this song months later. \r\n",
      "2015: many months later we both fell in love with this song. \r\n",
      "2016: Went to a JL concert with my girlfriend. \r\n",
      "2017: Sang this song and proposed to my girlfriend. \r\n",
      "2018: Got married and danced to this song at our wedding. \r\n",
      "2019: Still in love with this song. \r\n",
      "2020: so we're still together. And we're trying to start a family, But we're worried about when all this COVID-19 pandemic will end. \r\n",
      "2021: My wife got pregnant everyone!!\r\n",
      "2022: she’s due in July, I’m gonna be a dad!!!\r\n",
      "2023 i am a dad!!!(Thanks for all of your guys love)\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "If this song doesn't play at my wedding then I'm not coming.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "RIP Christine. As long as this song makes me cry you will never be forgotten my sista \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I don't know but when the first time I heard this song my heart and mind says it's for my father who broke me\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "I couldn't help myself crying \n",
      "Literally I can't stop \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I dedicate this song to someone who nearly broke me completely. Thank you God for having other plans for me. Thank you Kelly for this song.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Such a beautiful song. I dedicate this song to my fiance.\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I remember singing at the top of my lungs with my eyes balling when I was a little girl because I grew up with a narcissist as my parent and I related to this song so heavily… now I hear it for the first time in ages and I realized how much I’ve grown and healed. And listened to it without a single tear! I’ve fought for my respect after learning it was demanded of me while I never had any to begin with and defended others who were victims to mental, verbal, and physical abuse. I feel incredibly proud of myself, I didn’t realize how strong I became until now. Thank you, Kelly. \n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "I just wanted to say as a big fan since idol, you are such an inspiration for young girls and women everywhere! Your just keep being you and we will always love you for it Kelly! Xoxoxo\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "She wrote this when she was 16. Very powerful song to be written at any age, let alone as a teenager. I think sometimes we forget just how good a writer Kelly is. Sometimes her songwriting gets over shadowed by her unbelievably beautiful voice. We all have our own opinions about music, but for me Kelly is the best singer I've ever heard.\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "August 2019. A lot of memories are back\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "It is really wonderful and deserves the title of Golden Throat. It really became my favorite song, and thank you for this song \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "2012: cry\n",
      "2020: still crying\n",
      "Edit : 2021 still cry\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "keep playing this song out loud to my father, he just sang along lmao thanks for all the trauma you've given me\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "The story behind this song is amazing . One of Ed's closest friend Amy Wadge was running out of cash and she was unable to pay her mortgage . Ed let her co-write Thinking Out Loud and he paid for her task . This song changed the life of Ed and Amy . Ed man of words\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "It's not just a song, it's a feeling...\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "Reasons why you should stay alive.\n",
      "\n",
      "\n",
      "\n",
      "1. We would miss you. \n",
      "2. It's not worth the regret. Either by yourself if you failed or just simply left scars, or the regret everyone else feels by not doing enough to help you. \n",
      "3. It does get better. Believe it or not it will eventually get better. Sometimes you have to go through the storm to get to the rainbow. \n",
      "4. There's so much you would miss out on doing. \n",
      "5. There is always a reason to live. It might not be clear right now, but it is always there. \n",
      "6. So many people care, and it would hurt them if you hurt yourself. \n",
      "7. You ARE worth it. Don't let anyone, especially yourself, tell you otherwise. \n",
      "8. You are amazing. \n",
      "9. A time will come, once you've battled the toughest times of your life and are in ease once again, where you will be so glad that you decided to keep on living. You will emerge stronger from this all, and won't regret your choice to carry on with life. Because things always get better. \n",
      "10. What about all the things you've always wanted to do? What about the things you've planned, but never got around to doing? You can't do them when you're dead. \n",
      "11. I love you. Even if only one person loves you, that's still a reason to stay alive. \n",
      "12. You won't be able to listen to music if you die. \n",
      "13. Killing yourself is never worth it. You'll hurt both yourself and all the people you care about. \n",
      "14. There are so many people that would miss you, including me. \n",
      "15. You're preventing a future generation, YOUR KIDS, from even being born. \n",
      "16. How do you think your family would feel? Would it improve their lives if you died? \n",
      "17. You're gorgeous, amazing, and to someone you are perfect. \n",
      "18. Think about your favourite music artist, you'll never hear their voice again... \n",
      "19. You'll never have the feeling of walking into a warm building on a cold day \n",
      "20. Listening to incredibly loud music \n",
      "21. Being alive is just really good. \n",
      "22. Not being alive is really bad. \n",
      "23. Finding your soulmate. \n",
      "24. Red pandas \n",
      "25. Going to diners at three in the morning. \n",
      "26. Really soft pillows. \n",
      "27. Eating pizza in New York City. \n",
      "28. Proving people wrong with your success. \n",
      "29. Watching the jerks that doubted you fail at life. \n",
      "30. Seeing someone trip over a garbage can. \n",
      "31. Being able to help other people. \n",
      "32. Bonfires. \n",
      "33. Sitting on rooftops. \n",
      "34. Seeing every single country in the world. \n",
      "35. Going on roadtrips. \n",
      "36. You might win the lottery someday. \n",
      "37. Listening to music on a record player. \n",
      "38. Going to the top of the Eiffel Tower. \n",
      "39. Taking really cool pictures. \n",
      "40. Literally meeting thousands of new people. \n",
      "41. Hearing crazy stories. \n",
      "42. Telling crazy stories. \n",
      "43. Eating ice cream on a hot day. \n",
      "44. More Harry Potter books could come out, you never know. \n",
      "45. Travelling to another planet someday. \n",
      "46. Having an underwater house. \n",
      "47. Randomly running into your hero on the street. \n",
      "48. Having your own room at a fancy hotel. \n",
      "49. Trampolines. \n",
      "50. Think about your favourite movie, you'll never watch it again. \n",
      "51. Think about the feeling of laughing out loud in a public place because your best friend has just sent you an inside joke, \n",
      "52. Your survival will make the world better, even if it's for just one person or 20 or 100 or more. \n",
      "53. People do care. \n",
      "54. Treehouses \n",
      "55. Hanging out with your soul mate in a treehouse \n",
      "55. Snorting when you laugh and not caring who sees \n",
      "56. I don't even know you and I love you. \n",
      "57. I don't even know you and I care about you. \n",
      "58. Because nobody is going to be like you ever, so embrace your uniqueness! \n",
      "59. You won't be here to experience the first cat world emperor. \n",
      "60. WHAT ABOUT FOOD?! YOU'LL MISS CHOCOLATE AND ALL THE OTHER NOM THINGS! \n",
      "61. Starbucks. \n",
      "62. Hugs. \n",
      "63. Stargazing. \n",
      "64. You have a purpose, and it's up to you to find out what it is. \n",
      "65. You've changed somebody's life. \n",
      "66. Now you could change the world. \n",
      "67. You will meet the person that's perfect for you. \n",
      "68. No matter how much or how little, you have your life ahead of you. \n",
      "69. You have the chance to save somebody's life. \n",
      "70. If you end your life, you're stopping yourself from achieving great things. \n",
      "71. Making snow angels. \n",
      "72. Making snowmen. \n",
      "73. Snowball fights. \n",
      "74. Life is what you make of it. \n",
      "75. Everybody has a talent. \n",
      "76. Laughing until you cry. \n",
      "77. Having the ability to be sad means you have the ability to be happy. \n",
      "78. The world would not be the same if you didn't exist. \n",
      "79. Its possible to turn frowns, upside down \n",
      "80. Be yourself, don't take anyone's shit, and never let them take you alive. \n",
      "81. Heroes are ordinary people who make themselves extraordinary. Be your own hero. \n",
      "82. Being happy doesn't mean that everything is perfect. It means that you've decided to look beyond the imperfections. \n",
      "83. One day your smile will be real. \n",
      "84. Having a really hot, relaxing bath after a stressful day. \n",
      "85. Lying on grass and laughing at the clouds. \n",
      "86. Getting completely smashed with your best friends. \n",
      "87. Eating crazy food. \n",
      "88. Staying up all night watching your favourite films with a loved one. \n",
      "89. Sleeping in all day. \n",
      "90. Creating something you're proud of. \n",
      "91. You can look back on yourself 70 years later and being proud you didn't commit \n",
      "92. Being able to meet your Internet friends. \n",
      "93. Tea / Coffee / Hot Chocolate \n",
      "94. The new season of Sherlock\n",
      "95. Cuddling under the stars. \n",
      "96. Being stupid in public because you just can. \n",
      "97. If you are reading this then you are alive! Is there any more reason to smile? \n",
      "98. being able to hug that one person you havent seen in years \n",
      "99. People care enough about you and your future to come up with 100 reasons for you not to do this. \n",
      "100. But, the final and most important one is, just, being able to experience life. Because even if your life doesn't seem so great right now, literally anything could happen\n",
      "-\n",
      "\n",
      "From someone that cares about you <3\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Call me a hopeless romantic, for I'll agree, this is such a touching, romantic song! What woman wouldn't want their true love to sing them this song!!!\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "this will be the song I dance to at my wedding\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "This helped me through every break up in my life.\n",
      "++++++++++\n",
      "Annotation Label: 0\n",
      "It’s sad when your parents don’t want each other anymore. It’s even sadder when they don’t want you, either\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "“My heart can’t possibly break when it wasn’t even whole to start with” so profound !\n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "Once again listening this year as well during valentine week thinking about my Crush \n",
      "Don't you guys think about your crush as well?! \n",
      "++++++++++\n",
      "Annotation Label: 1\n",
      "You know a song is legendary when after almost 10 years it still makes you cry\n",
      "++++++++++\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#evaluate model\n",
    "\n",
    "# reload model (if needed)\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "\n",
    "# start evaluation\n",
    "print(\"Starting Evaluation:\")\n",
    "result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "\n",
    "# print results\n",
    "print(\"Results:\")\n",
    "for i in result:\n",
    "    print(i, result[i])\n",
    "\n",
    "#tp = true positives\n",
    "#tn = true negatives\n",
    "#fp = false positives\n",
    "#fn = false negaives\n",
    "#auroc = precision\n",
    "#auprc = average precison\n",
    "    \n",
    "print(\"Precision:\")\n",
    "print((len(eval_df)-len(wrong_predictions))/len(eval_df))\n",
    "    \n",
    "print(len(wrong_predictions))\n",
    "\n",
    "# print all wrong preductions (if wanted)\n",
    "for n, dic in enumerate(wrong_predictions):\n",
    "    print(\"Annotation Label:\", dic.label)\n",
    "    print(dic.text_a)\n",
    "    print(10*'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825858c",
   "metadata": {},
   "source": [
    "# Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6768fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0.2 Unnamed: 0.1 Unnamed: 0            author  \\\n",
      "10274         10274            0          0     Bruce VanBeek   \n",
      "10275         10275            3          3         nayneeeee   \n",
      "10276         10276            4          4  letitia marshall   \n",
      "10277         10277            7          7           soni uk   \n",
      "10278         10278            8          8  Christian Havana   \n",
      "...             ...          ...        ...               ...   \n",
      "21017         21017        20080      20080          ScarWølf   \n",
      "21018         21018        20081      20081            Ronald   \n",
      "21019         21019        20082      20082         Rahaf Mrd   \n",
      "21020         21020        20085      20085           Bela S.   \n",
      "21021         21021        20086      20086    shrooq mujarbi   \n",
      "\n",
      "                                                 comment  \\\n",
      "10274  I know a daughter of a good man . Since her mo...   \n",
      "10275  In every failed relationship it was always the...   \n",
      "10276                                     child helpline   \n",
      "10277                         22/06/2023 United Kingdom    \n",
      "10278                                  My childhood song   \n",
      "...                                                  ...   \n",
      "21017  When you have depression and you listen to thi...   \n",
      "21018      31mil people cried in the middle of the night   \n",
      "21019  I am sorry for my children but I got married a...   \n",
      "21020                      This songs describes my life.   \n",
      "21021  I learned to play on the safe side to not get ...   \n",
      "\n",
      "                              origin  origin_status lang  time_stamp  \n",
      "10274  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10275  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10276  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10277  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "10278  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "...                              ...            ...  ...         ...  \n",
      "21017  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21018  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21019  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21020  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "21021  Kelly_Clarkson-Because_Of_You            1.0   en       False  \n",
      "\n",
      "[10631 rows x 9 columns]\n",
      "10631\n"
     ]
    }
   ],
   "source": [
    "# prepare Application-Dataset (input needs to be a list)\n",
    "filename = \"comment_downloads/data_combined_53221.tsv\"\n",
    "origin = \"Kelly_Clarkson-Because_Of_You\"\n",
    "time_stamp = False\n",
    "\n",
    "\n",
    "apply_df = pd.read_csv(filename, sep=\"\\t\")\n",
    "apply_df = apply_df[apply_df[\"origin\"] == origin]\n",
    "apply_df = apply_df[apply_df[\"time_stamp\"] == time_stamp]\n",
    "\n",
    "\n",
    "\n",
    "print(apply_df)\n",
    "apply_lst = apply_df['comment'].tolist()\n",
    "\n",
    "print(len(apply_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69f880b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                                                                    | 1/113 [00:04<09:03,  4.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:04<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "\n",
    "#apply model\n",
    "predictions, raw_outputs = model.predict(apply_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fae804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "comment_downloads/data_combined_53221.tsv Kelly_Clarkson-Because_Of_You True\n",
      "1 = schnulzig; 0 = nicht-schnulzig\n",
      "0    89\n",
      "1    24\n",
      "Name: prediction, dtype: int64\n",
      "results saved in comment_downloads/data_combined_53221.tsv\n",
      "21.24% der Kommentare wurden als schnulzig klassifiziert.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#sys.stdout = open('output.txt','a')\n",
    "print(20*\"-\")\n",
    "print(filename, origin, time_stamp)\n",
    "\n",
    "# output and save predictions\n",
    "final = []\n",
    "for i, prediction in enumerate(predictions):\n",
    "  comment_result = (apply_lst[i], str(prediction), str(raw_outputs[i]))\n",
    "  final.append(comment_result)\n",
    "df = pd.DataFrame(final, columns =['txt', 'prediction', 'probablility'])\n",
    "\n",
    "#filename = 'output.tsv'\n",
    "#df.to_csv(filename, sep='\\t')\n",
    "counts = df[\"prediction\"].value_counts()\n",
    "print(\"1 = schnulzig; 0 = nicht-schnulzig\")\n",
    "print(counts)\n",
    "print(f\"results saved in {filename}\")\n",
    "\n",
    "perc = counts.iloc[1]/(counts.iloc[0]+counts.iloc[1])\n",
    "perc = '{:,.2%}'.format(perc)\n",
    "print(f\"{perc} der Kommentare wurden als schnulzig klassifiziert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a53ceb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_p82/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model_p82/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model_p82.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                | 0/1 [00:00<?, ?it/s]Process SpawnPoolWorker-105:\n",
      "Process SpawnPoolWorker-109:\n",
      "Process SpawnPoolWorker-103:\n",
      "Process SpawnPoolWorker-108:\n",
      "Process SpawnPoolWorker-107:\n",
      "Process SpawnPoolWorker-104:\n",
      "Process SpawnPoolWorker-101:\n",
      "Process SpawnPoolWorker-106:\n",
      "Process SpawnPoolWorker-110:\n",
      "  0%|                                                                                                                                                                                                | 0/1 [00:05<?, ?it/s]Process SpawnPoolWorker-102:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/a\n",
      "naconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m comment \u001b[38;5;241m=\u001b[39m single_lst\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 47\u001b[0m predictions, raw_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m com_lst\u001b[38;5;241m.\u001b[39mappend(comment)\n\u001b[1;32m     49\u001b[0m pred_lst\u001b[38;5;241m.\u001b[39mappend(predictions)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:2087\u001b[0m, in \u001b[0;36mClassificationModel.predict\u001b[0;34m(self, to_predict, multi_label)\u001b[0m\n\u001b[1;32m   2085\u001b[0m         out_label_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(eval_dataset)))\n\u001b[1;32m   2086\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2087\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2091\u001b[0m eval_sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(eval_dataset)\n\u001b[1;32m   2092\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m   2093\u001b[0m     eval_dataset, sampler\u001b[38;5;241m=\u001b[39meval_sampler, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size\n\u001b[1;32m   2094\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1827\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1827\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mClassificationDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1833\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_cache\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:249\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    243\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    244\u001b[0m             (text_a[i : i \u001b[38;5;241m+\u001b[39m chunksize], \u001b[38;5;28;01mNone\u001b[39;00m, tokenizer, args\u001b[38;5;241m.\u001b[39mmax_seq_length)\n\u001b[1;32m    245\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_a), chunksize)\n\u001b[1;32m    246\u001b[0m         ]\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(args\u001b[38;5;241m.\u001b[39mprocess_count) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m--> 249\u001b[0m         examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m                \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_data_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     examples \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    258\u001b[0m         key: torch\u001b[38;5;241m.\u001b[39mcat([example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples])\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_ml_env2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import sys\n",
    "#machine learning packages\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "# https://pypi.org/project/simpletransformers/ <- source evaluation tipps\n",
    "import pandas as pd\n",
    "from sys import argv\n",
    "from sys import stderr\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "origin = \"Kelly_Clarkson-Because_Of_You\"\n",
    "time_stamp = False\n",
    "\n",
    "\n",
    "\n",
    "# prepare Application-Dataset (input needs to be a list)\n",
    "filename = \"comment_downloads/data_combined_53221.tsv\"\n",
    "apply_df = pd.read_csv(filename, sep=\"\\t\")\n",
    "apply_df = apply_df[apply_df[\"origin\"] == origin]\n",
    "apply_df = apply_df[apply_df[\"time_stamp\"] == time_stamp]\n",
    "apply_lst = apply_df['comment'].tolist()\n",
    "apply_lst = apply_lst[:5]\n",
    "\n",
    "print(len(apply_lst))\n",
    "# load model\n",
    "model = ClassificationModel('distilbert','model_p82', use_cuda=False)\n",
    "#apply model\n",
    "\n",
    "com_lst = []\n",
    "pred_lst = []\n",
    "\n",
    "for i, comment in enumerate(apply_lst):\n",
    "    single_lst = []\n",
    "    single_lst.append(comment)\n",
    "    comment = single_lst\n",
    "    print(i)\n",
    "    predictions, raw_outputs = model.predict(comment)\n",
    "    com_lst.append(comment)\n",
    "    pred_lst.append(predictions)\n",
    "\n",
    "print(20*\"-\")\n",
    "print(filename, origin, time_stamp)\n",
    "\n",
    "# output and save predictions\n",
    "final = []\n",
    "for i, comment in enumerate(com_lst):\n",
    "  comment_row = (comment, str(pred_lst[i]))\n",
    "  final.append(comment_row)\n",
    "\n",
    "df = pd.DataFrame(final, columns =['txt', 'prediction'])\n",
    "\n",
    "print(\"1 = schnulzig; 0 = nicht-schnulzig\")\n",
    "counts = df[\"prediction\"].value_counts()\n",
    "print(counts)\n",
    "try:\n",
    "    perc = counts.iloc[1]/(counts.iloc[0]+counts.iloc[1])\n",
    "except:\n",
    "    perc = 0\n",
    "perc = '{:,.2%}'.format(perc)\n",
    "print(f\"{perc} der Kommentare wurden als schnulzig klassifiziert.\")\n",
    "\n",
    "\n",
    "filename = 'output.tsv'\n",
    "df.to_csv(filename, sep='\\t')\n",
    "print(f\"results saved in {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_ml_env2)",
   "language": "python",
   "name": "my_ml_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
